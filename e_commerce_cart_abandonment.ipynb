{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# E-Commerce Cart Abandonment Prediction\n",
    "\n",
    "- Name: **Fahrettin Ege Bilge**\n",
    "- ID: **21070001052**\n",
    "- Instructor: **Assoc. Prof. Dr. Ömer ÇETİN**\n",
    "\n",
    "## Table Of Contents\n",
    "\n",
    "1. [Introduction](#Introduction)\n",
    "2. [Preprocessing](#Preprocessing)\n",
    "    - [Overview of Preprocessing Steps](#Overview-of-Preprocessing-Steps)\n",
    "3. [Split Dataset](#Split-Dataset)\n",
    "4. [Machine Learning Algorithm Selection](#Machine-Learning-Algorithm-Selection)\n",
    "    - [1. Logistic Regression: A baseline algorithm for binary classification.](#1.-Logistic-Regression:-A-baseline-algorithm-for-binary-classification.)\n",
    "    - [2.K-Nearest Neighbors (KNN): A distance-based classification model.](#2.K-Nearest-Neighbors-(KNN):-A-distance-based-classification-model.)\n",
    "    - [3. Support Vector Machines (SVM): Clear decision boundary for classification tasks.](#3.-Support-Vector-Machines-(SVM):-Clear-decision-boundary-for-classification-tasks.)\n",
    "    - [4. Naive Bayes: Complements one-hot encoding.](#4.-Naive-Bayes:-Complements-one-hot-encoding.)\n",
    "5. [Evaluation Metrics](#Evaluation-Metrics)\n",
    "    - [Accuracy](#Accuracy)\n",
    "    - [Precision](#Precision)\n",
    "    - [Recall](#Recall)\n",
    "    - [F1-Score](#F1-Score)\n",
    "    - [Confusion Matrix](#Confusion-Matrix)\n",
    "6. [Results](#Results)\n",
    "7. [Conclusion](#Conclusion)\n",
    "8. [Appendices](#Appendices)\n",
    "9. [References](#References)\n",
    "\n",
    "\n",
    "## Introduction\n",
    "Predicting cart abandonment is crucial for e-commerce platforms to reduce lost revenue and improve user experience. This project uses supervised learning techniques to predict the likelihood of users abandoning their carts based on features like cart contents, payment methods, and purchase history."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "### Overview of Preprocessing Steps\n",
    "1. Filtering Relevant Rows:\n",
    "- Rows with status values other than canceled and complete were removed.\n",
    "- Justification: canceled maps to abandoned = 1, while complete maps to abandoned = 0. Other statuses do not provide relevant information for this task.\n",
    "2. Handling Categorical Features:\n",
    "- Categorical variables (category_name_1 and payment_method) were one-hot encoded.\n",
    "- Justification: One-hot encoding ensures that these variables are represented in a format suitable for machine learning models without assuming any ordinal relationship.\n",
    "3. Handling Numerical Features:\n",
    "- Numerical features (price, grand_total, discount_amount, total_purchases, and total_orders) were scaled using MinMaxScaler.\n",
    "- Justification: Scaling ensures that all features are normalized, preventing features with large magnitudes from dominating the model.\n",
    "4. Outlier Handling:\n",
    "- Numerical columns were clipped at the 95th percentile to mitigate the effect of outliers.\n",
    "- Justification: Outliers can disproportionately influence certain machine learning models like Logistic Regression or KNN.\n",
    "5. Tracking Customer History:\n",
    "- Aggregated total_purchases (sum of grand_total for each customer) and total_orders (number of orders per customer) were added as features.\n",
    "- Justification: These features provide insights into customer behavior and engagement, which are critical for predicting cart abandonment.\n",
    "6. Balancing the Dataset:\n",
    "- Undersampling was used to balance the dataset by ensuring equal representation of abandoned (1) and not abandoned (0) classes.\n",
    "- Justification: An imbalanced dataset can bias the model towards the majority class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import helper as hlp\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kaggle_dataset = 'data/kaggle_dataset/Pakistan_Largest_Ecommerce_Dataset.csv'\n",
    "preprocessed_dataset = 'data/preprocessed_dataset/preprocessed_dataset.csv'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess of Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hlp.preprocess_dataset(kaggle_dataset, preprocessed_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization Of Preprocessed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hlp.visualize_preprocessed_data(preprocessed_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Dataset\n",
    "To train and evaluate machine learning models effectively, the dataset is split into training and testing subsets. This ensures that the model is trained on one portion of the data and evaluated on unseen data to measure its performance. \n",
    "\n",
    "#### Steps:\n",
    "1. **Train-Test Split**: \n",
    "   - The dataset is split into 80% training data and 20% testing data.\n",
    "2. **Stratification**: \n",
    "   - Stratified splitting ensures that the class distribution (abandoned vs. not abandoned) is preserved in both the training and testing datasets.\n",
    "3. **Random State**: \n",
    "   - Setting a `random_state` ensures reproducibility of results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "df = pd.read_csv(preprocessed_dataset) \n",
    "\n",
    "# Ensure 'abandoned' is in the dataframe (required for splitting)\n",
    "if 'abandoned' not in df.columns:\n",
    "    raise ValueError(\"'abandoned' column is missing. Ensure preprocessing includes this target variable.\")\n",
    "\n",
    "# Define features (X) and target variable (y)\n",
    "X = df.drop(columns=['abandoned'])  # Features\n",
    "y = df['abandoned']                # Target variable\n",
    "\n",
    "# Split the dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, \n",
    "    test_size=0.2,          # 20% of the data for testing\n",
    "    stratify=y,             # Preserve class distribution\n",
    "    random_state=42         # Ensure reproducibility\n",
    ")\n",
    "\n",
    "# Display shapes and class distribution\n",
    "print(\"Dataset Split Information:\")\n",
    "print(f\"Training data shape: {X_train.shape} (Features), {y_train.shape} (Target)\")\n",
    "print(f\"Testing data shape: {X_test.shape} (Features), {y_test.shape} (Target)\")\n",
    "\n",
    "# Display class distribution in training and testing sets\n",
    "train_class_distribution = y_train.value_counts(normalize=True).to_dict()\n",
    "test_class_distribution = y_test.value_counts(normalize=True).to_dict()\n",
    "\n",
    "print(\"\\nClass Distribution in Training Data:\")\n",
    "for class_label, proportion in train_class_distribution.items():\n",
    "    print(f\"  Class {class_label}: {proportion:.2%}\")\n",
    "\n",
    "print(\"\\nClass Distribution in Testing Data:\")\n",
    "for class_label, proportion in test_class_distribution.items():\n",
    "    print(f\"  Class {class_label}: {proportion:.2%}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning Algorithm Selection\n",
    "Justification for Algorithm Choices\n",
    "\n",
    "1. Logistic Regression:\n",
    "- Chosen for its simplicity, interpretability, and efficiency on linearly separable data.\n",
    "- It also provides probabilities for predictions, making it suitable for understanding the likelihood of cart abandonment.\n",
    "2. K-Nearest Neighbors (KNN):\n",
    "- A non-parametric algorithm that uses similarity measures to make predictions.\n",
    "- Effective for capturing local patterns and relationships in the data.\n",
    "3. Support Vector Machines (SVM):\n",
    "- Robust to high-dimensional spaces and outliers, making it a good choice for scaled numerical features.\n",
    "- Provides a clear decision boundary for classification tasks.\n",
    "4. Naive Bayes:\n",
    "- Efficient and works well for categorical data due to its assumption of feature independence.\n",
    "- It complements the one-hot encoded categorical features in our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize models\n",
    "logistic_regression = LogisticRegression(random_state=42)\n",
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "svm = LinearSVC(random_state=42, max_iter=100000,verbose=True)\n",
    "naive_bayes = GaussianNB()\n",
    "# Add ANN (MLPClassifier) to the pipeline\n",
    "ann = MLPClassifier(\n",
    "    hidden_layer_sizes=(128, 64),  # Two hidden layers with 128 and 64 neurons\n",
    "    activation='relu',            # ReLU activation function\n",
    "    solver='adam',                # Adam optimizer\n",
    "    max_iter=200,                 # Maximum number of iterations\n",
    "    random_state=42               # Reproducibility\n",
    ")\n",
    "random_forest = RandomForestClassifier(n_estimators=100, max_depth=None,random_state=42)\n",
    "\n",
    "# Store results\n",
    "results = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Logistic Regression: A baseline algorithm for binary classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.append(hlp.evaluate_model(logistic_regression, X_train, y_train, X_test, y_test, \"Logistic Regression\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.K-Nearest Neighbors (KNN): A distance-based classification model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.append(hlp.evaluate_model(knn, X_train, y_train, X_test, y_test, \"K-Nearest Neighbors\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Support Vector Machines (SVM): Clear decision boundary for classification tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.append(hlp.evaluate_model(svm, X_train, y_train, X_test, y_test, \"Support Vector Machines\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Naive Bayes: Complements one-hot encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.append(hlp.evaluate_model(naive_bayes, X_train, y_train, X_test, y_test, \"Naive Bayes\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.ANN: A costly solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.append(hlp.evaluate_model(ann, X_train, y_train, X_test, y_test, \"Artificial Neural Network (ANN)\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.append(hlp.evaluate_model(random_forest, X_train, y_train, X_test, y_test, \"Random Forest\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation Metrics\n",
    "\n",
    "- **Accuracy**: Overall correctness of predictions.\n",
    "- **Precision**: Ratio of true positive predictions to all positive predictions.\n",
    "- **Recall**: Ratio of true positives to all actual positives.\n",
    "- **F1 Score**: Harmonic mean of precision and recall.\n",
    "- **Confusion Matrix**: Visualization of classification performance.\n",
    "\n",
    "#### Accuracy\n",
    "$$\n",
    "\\text{Accuracy} = \\frac{\\text{TP} + \\text{TN}}{\\text{TP} + \\text{TN} + \\text{FP} + \\text{FN}}\n",
    "$$\n",
    "\n",
    "#### Precision\n",
    "$$\n",
    "\\text{Precision} = \\frac{\\text{TP}}{\\text{TP} + \\text{FP}}\n",
    "$$\n",
    "\n",
    "#### Recall\n",
    "$$\n",
    "\\text{Recall} = \\frac{\\text{TP}}{\\text{TP} + \\text{FN}}\n",
    "$$\n",
    "\n",
    "#### F1-Score\n",
    "$$\n",
    "F_1 = 2 \\cdot \\frac{\\text{Precision} \\cdot \\text{Recall}}{\\text{Precision} + \\text{Recall}}\n",
    "$$\n",
    "\n",
    "#### Confusion Matrix\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "\\text{TP} & \\text{FP} \\\\\n",
    "\\text{FN} & \\text{TN}\n",
    "\\end{bmatrix}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame for results\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "- Comparison of model performance on test data.\n",
    "- Discussion of strengths and weaknesses of each algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract metrics from results\n",
    "performance_metrics = ['Accuracy', 'Precision', 'Recall', 'F1-Score']\n",
    "filtered_metrics_df = results_df[['Model'] + performance_metrics]\n",
    "\n",
    "# Plot performance metrics\n",
    "filtered_metrics_df.set_index('Model').plot(kind='bar', figsize=(12, 6), alpha=0.8)\n",
    "plt.title(\"Model Performance Metrics\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.xlabel(\"Model\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.legend(loc='lower right', title=\"Metrics\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot confusion matrices side by side\n",
    "fig, axes = plt.subplots(1, len(results), figsize=(20, 6), constrained_layout=True)\n",
    "\n",
    "for i, result in enumerate(results):\n",
    "    cm = result['Confusion Matrix']\n",
    "    model_name = result['Model']\n",
    "    \n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False, \n",
    "                xticklabels=['Not Abandoned', 'Abandoned'], \n",
    "                yticklabels=['Not Abandoned', 'Abandoned'], ax=axes[i])\n",
    "    \n",
    "    axes[i].set_title(f\"{model_name}\\nConfusion Matrix\")\n",
    "    axes[i].set_ylabel('True Labels' if i == 0 else '')  # Only the first plot has the y-axis label\n",
    "    axes[i].set_xlabel('Predicted Labels')\n",
    "\n",
    "plt.suptitle(\"Confusion Matrices of All Models\", fontsize=16)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for grouped bar chart\n",
    "metrics_melted = pd.melt(\n",
    "    results_df, id_vars='Model', \n",
    "    value_vars=['Accuracy', 'Precision', 'Recall', 'F1-Score'],\n",
    "    var_name='Metric', value_name='Score'\n",
    ")\n",
    "\n",
    "# Plot grouped bar chart\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(data=metrics_melted, x='Metric', y='Score', hue='Model', palette='Set2')\n",
    "plt.title(\"Performance Metrics by Model\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.xlabel(\"Metric\")\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5), constrained_layout=True)\n",
    "\n",
    "# Training Time\n",
    "results_df.plot(kind='bar', x='Model', y='Training Time (s)', ax=axes[0], color='skyblue', legend=False, logy=True)\n",
    "axes[0].set_title('Training Time (Log Scale)')\n",
    "axes[0].set_ylabel('Time (s, log scale)')\n",
    "axes[0].set_xlabel('Model')\n",
    "axes[0].grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "# Evaluation Time\n",
    "results_df.plot(kind='bar', x='Model', y='Evaluation Time (s)', ax=axes[1], color='lightgreen', legend=False, logy=True)\n",
    "axes[1].set_title('Evaluation Time (Log Scale)')\n",
    "axes[1].set_ylabel('Time (s, log scale)')\n",
    "axes[1].set_xlabel('Model')\n",
    "axes[1].grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "# Memory Usage\n",
    "results_df.plot(kind='bar', x='Model', y='Memory Used (MB)', ax=axes[2], color='salmon', legend=False, logy=True)\n",
    "axes[2].set_title('Memory Usage (Log Scale)')\n",
    "axes[2].set_ylabel('Memory (MB, log scale)')\n",
    "axes[2].set_xlabel('Model')\n",
    "axes[2].grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "plt.suptitle(\"Model Training and Evaluation Performance (Log Scale)\", fontsize=16)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "Summarize findings, highlight key insights, and suggest potential improvements for future work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendices\n",
    "- Sample code snippets.\n",
    "## References\n",
    "- Dataset: https://www.kaggle.com/datasets/zusmani/pakistans-largest-ecommerce-dataset/data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
